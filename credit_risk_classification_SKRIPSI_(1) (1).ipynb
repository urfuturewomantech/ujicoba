{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eflo8s1VU6mM"
      },
      "source": [
        "# Analisa Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfZrMY-GBMwx"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/credit_risk_dataset.csv')\n",
        "\n",
        "# Preprocessing missing values\n",
        "df['loan_int_rate'].fillna(df['loan_int_rate'].mean(), inplace=True)\n",
        "df['person_emp_length'].fillna(round(df['person_emp_length'].mean()), inplace=True)\n",
        "\n",
        "# Define features and label\n",
        "features = [\n",
        "    'person_income', 'loan_grade', 'loan_int_rate', 'loan_percent_income',\n",
        "    'person_home_ownership', 'cb_person_default_on_file'\n",
        "]\n",
        "target = 'loan_status'\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define numeric and categorical features\n",
        "numeric_features = ['person_income', 'loan_int_rate', 'loan_percent_income']\n",
        "categorical_features = ['loan_grade', 'person_home_ownership', 'cb_person_default_on_file']\n",
        "\n",
        "# Create preprocessor\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', MinMaxScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create full pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessing', preprocessor),\n",
        "    ('classifier', DecisionTreeClassifier(criterion='entropy', max_depth=8, random_state=0))\n",
        "])\n",
        "\n",
        "# Fit pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Save pipeline\n",
        "joblib.dump(pipeline, 'pipeline_model.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWxpcdS1GwIE"
      },
      "outputs": [],
      "source": [
        "#Untuk memberi tampilan yang menarik\n",
        "class color:\n",
        "    PURPLE = '\\033[95m'\n",
        "    CYAN = '\\033[96m'\n",
        "    DARKCYAN = '\\033[36m'\n",
        "    BLUE = '\\033[94m'\n",
        "    GREEN = '\\033[92m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    RED = '\\033[91m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "    GREYBACKBLACK =  '\\033[0;30;47m'\n",
        "    END = '\\033[0m'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT1m5FvbBSvG"
      },
      "outputs": [],
      "source": [
        "#menampilkan cuplikan data\n",
        "print(color.BOLD + color.GREYBACKBLACK + \"Berikut adalah dataset yang akan kita gunakan\"+color.END)\n",
        "display(df.head())\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GMYbgA-iLgR"
      },
      "outputs": [],
      "source": [
        "#menampilkan cuplikan data\n",
        "print(color.BOLD + color.GREYBACKBLACK + \"Berikut adalah dataset yang akan kita gunakan\"+color.END)\n",
        "display(df.info())\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqe3hDEABhds"
      },
      "outputs": [],
      "source": [
        "#menampilkan ukuran dari data csv\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1N849lnCPkv"
      },
      "outputs": [],
      "source": [
        "#menampilkan semua nama kolom pada csv\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLgX2xgLCPw-"
      },
      "outputs": [],
      "source": [
        "#menampilkan semua karakteristik statistik pada data csv\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK-PavMSUvxN"
      },
      "source": [
        "# Missing Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oPUlbJcHYlF"
      },
      "outputs": [],
      "source": [
        "#memeriksa apakah terdapat missing value\n",
        "print(color.BOLD+\"Periksa isi data apakah sudah siap digunakan\"+color.END)\n",
        "print(\"Terdapat data hilang? \",df.isnull().values.any())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ySMct-HHYxr"
      },
      "outputs": [],
      "source": [
        "#mencari jumlah missing value pda data\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPhl-1_hHtoU"
      },
      "outputs": [],
      "source": [
        "#mengubah missing value loan_int_rate menjadi rata-rata\n",
        "loanmean = df['loan_int_rate'].mean()\n",
        "df['loan_int_rate'] = df['loan_int_rate'].fillna(loanmean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kFSIHsRJhsU"
      },
      "outputs": [],
      "source": [
        "#mengubah missing value person_emp_length menjadi rata-rata\n",
        "persempmean = round(df['person_emp_length'].mean())\n",
        "df['person_emp_length'] = df['person_emp_length'].fillna(persempmean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nYu2aQIJdWv"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTCxes5a17JU"
      },
      "outputs": [],
      "source": [
        "# menampilkan informasi tiap kolom\n",
        "df['person_emp_length']=df['person_emp_length'].astype('int64')\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVmksMjEeY4S"
      },
      "outputs": [],
      "source": [
        "#menampilkan semua karakteristik statistik pada data csv setelah missing value ditangani\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GOFUIuLUrh-"
      },
      "source": [
        "# Memeriksa Data Pengganggu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDFZvVIuMIHu"
      },
      "outputs": [],
      "source": [
        "#periksa jenis data bukan numerik untuk melihat terdapat data mengganggu atau tidak\n",
        "display(df.person_home_ownership.unique())\n",
        "display(df.loan_intent.unique())\n",
        "display(df.loan_grade.unique())\n",
        "display(df.cb_person_default_on_file.unique())\n",
        "display(df.loan_status.unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxfqraomUXrw"
      },
      "source": [
        "# Penanganan Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIQvMZe_Qn6B"
      },
      "outputs": [],
      "source": [
        "#mendeteksi outlier\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "dfq = ['person_age','person_income','person_emp_length','loan_amnt','loan_int_rate','loan_percent_income','cb_person_cred_hist_length']\n",
        "plt.figure(figsize=(10,5))\n",
        "for i in range(0,len(dfq)):\n",
        "    plt.subplot(1,len(dfq),i+1)\n",
        "    sns.boxplot(y=df[dfq[i]])\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9sPLFwyq9pS"
      },
      "outputs": [],
      "source": [
        "df.loc[df['person_age'] >100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuNOVtMXq4d8"
      },
      "outputs": [],
      "source": [
        "#menghapus row\n",
        "df.drop([81,183,575,747], axis=0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiyJ5IxjSMEL"
      },
      "outputs": [],
      "source": [
        "df.loc[df['person_emp_length'] >df['person_age'] ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5w1-Fp8hrv3F"
      },
      "outputs": [],
      "source": [
        "#menghapus row\n",
        "df.drop([0,210], axis=0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBoHnst9sFjE"
      },
      "outputs": [],
      "source": [
        "#mendeteksi outlier\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "dfq = ['person_age','person_income','person_emp_length','loan_amnt','loan_int_rate','loan_percent_income','cb_person_cred_hist_length']\n",
        "plt.figure(figsize=(10,5))\n",
        "for i in range(0,len(dfq)):\n",
        "    plt.subplot(1,len(dfq),i+1)\n",
        "    sns.boxplot(y=df[dfq[i]])\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I4UvYn8UPiG"
      },
      "source": [
        "# Visualisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUyXEh3LI83P"
      },
      "outputs": [],
      "source": [
        "import warnings # tambahan agar tidak muncul warnings saat running\n",
        "warnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-_kygP5IWS2"
      },
      "outputs": [],
      "source": [
        "# First, make sure df is defined by importing your data\n",
        "# For example: df = pd.read_csv('your_data.csv')\n",
        "\n",
        "# membuat plot untuk melihat visualisasi data\n",
        "dfcount=['person_age','person_home_ownership','person_emp_length','loan_intent','loan_grade','loan_status','cb_person_default_on_file','cb_person_cred_hist_length']\n",
        "dfhisto=['person_income','loan_amnt','loan_int_rate','loan_percent_income']\n",
        "\n",
        "# Import required libraries if not already imported\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Check if columns exist in the DataFrame before plotting\n",
        "for i in range(0, len(dfcount)):\n",
        "    if dfcount[i] in df.columns:  # Check if column exists\n",
        "        plt.figure(figsize=(25,5))\n",
        "        sns.countplot(x=df[dfcount[i]])  # Specify x parameter explicitly\n",
        "        plt.title(f'Count plot of {dfcount[i]}')\n",
        "        plt.xticks(rotation=45)  # Rotate labels for better readability\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Column '{dfcount[i]}' not found in DataFrame\")\n",
        "\n",
        "for i in range(0, len(dfhisto)):\n",
        "    if dfhisto[i] in df.columns:  # Check if column exists\n",
        "        plt.figure(figsize=(25,5))\n",
        "        sns.histplot(df[dfhisto[i]], kde=True)\n",
        "        plt.title(f'Histogram of {dfhisto[i]}')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Column '{dfhisto[i]}' not found in DataFrame\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq068af9zXI7"
      },
      "outputs": [],
      "source": [
        "df_tidakberhasil = df[df['loan_status'] == 1]\n",
        "# membuat plot untuk melihat visualisasi variabel-variabel terhadap loan_status tidak berhasil\n",
        "dfcount=['person_age','person_home_ownership','person_emp_length','loan_intent','loan_grade','cb_person_default_on_file','cb_person_cred_hist_length']\n",
        "dfhisto=['person_income','loan_amnt','loan_int_rate','loan_percent_income']\n",
        "\n",
        "# Fix for countplot - use x parameter to specify column name\n",
        "for i in range(0, len(dfcount)):\n",
        "    plt.figure(figsize=(25,5))\n",
        "    sns.countplot(x=dfcount[i], data=df_tidakberhasil)  # Use x parameter and data parameter\n",
        "    plt.show()\n",
        "\n",
        "# Fix for histplot - use x parameter to specify column name\n",
        "for i in range(0, len(dfhisto)):\n",
        "    plt.figure(figsize=(25,5))\n",
        "    sns.histplot(x=dfhisto[i], data=df_tidakberhasil, kde=True)  # Use x parameter and data parameter\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PMLdt-12Xl1"
      },
      "source": [
        "# Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaNBecpJ17Jz"
      },
      "outputs": [],
      "source": [
        "df.select_dtypes(include=['object'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3ymhOP517J0"
      },
      "outputs": [],
      "source": [
        "# OneHot encoding\n",
        "i = ['person_home_ownership', 'loan_intent', 'cb_person_default_on_file']\n",
        "OneHot = pd.get_dummies(df, columns = i)\n",
        "df = OneHot\n",
        "\n",
        "# Ordinal encoding\n",
        "df.loan_grade = pd.Categorical(df.loan_grade)\n",
        "df['loan_grade'] = df.loan_grade.cat.codes\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW1wo-P617J1"
      },
      "outputs": [],
      "source": [
        "#Melihat info dataframe tidak ada yang object\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsmB1GJJ_g-R"
      },
      "source": [
        "# Uji Multikolinearitas dengan Variance Inflation Factor (VIF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb0-0FF517KR"
      },
      "outputs": [],
      "source": [
        "# feature selection with VIF\n",
        "# VIF (Variance Inflation Factor) checking\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import pandas as pd\n",
        "\n",
        "# Select only numerical columns from the dataframe\n",
        "# This is the key fix - we need to exclude non-numerical columns\n",
        "numerical_df = df.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Calculate VIF only for numerical columns\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['feature'] = numerical_df.columns\n",
        "vif_data['VIF'] = [variance_inflation_factor(numerical_df.values, i) for i in range(len(numerical_df.columns))]\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# column's vif\n",
        "vif_data.sort_values(by='VIF', ascending=False)  # batas vif = 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79AKRlz36TI9"
      },
      "source": [
        "# Feature Selection dengan Pearson Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SW9xzqrZ17J2"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMB1K7GO17J3"
      },
      "outputs": [],
      "source": [
        "#meletakkan variabel target di paling akhir\n",
        "df = df[['person_age', 'person_income', 'person_emp_length', 'loan_grade',\n",
        "       'loan_amnt', 'loan_int_rate', 'loan_percent_income',\n",
        "       'cb_person_cred_hist_length', 'person_home_ownership_MORTGAGE',\n",
        "       'person_home_ownership_OTHER', 'person_home_ownership_OWN',\n",
        "       'person_home_ownership_RENT', 'loan_intent_DEBTCONSOLIDATION',\n",
        "       'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT',\n",
        "       'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE',\n",
        "       'cb_person_default_on_file_N', 'cb_person_default_on_file_Y', 'loan_status']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBFJVbX817J4"
      },
      "outputs": [],
      "source": [
        "# Feature Selection (Pearson Correlation)\n",
        "print(color.BOLD + color.BLUE + 'Dilakukan seleksi fitur dengan Pearson Correlation' + color.END)\n",
        "plt.figure(figsize = (15,15))\n",
        "korelasi = df.corr()\n",
        "display(korelasi)\n",
        "sns.heatmap(korelasi, annot = True, cmap = plt.cm.Blues)\n",
        "plt.show()\n",
        "\n",
        "# Akan dicari variabel-variabel yang berkorelasi dengan variabel target (>rata-rata seluruh korelasi)\n",
        "print(color.BOLD + color.RED + 'Didapat nilai absolut korelasi antar variabel dengan variabel output adalah sebagai berikut' + color.END)\n",
        "print(abs(korelasi['loan_status'][1:-1]))\n",
        "korelasi['loan_status'] = korelasi['loan_status'][1:-1]\n",
        "bataskor = abs(korelasi['loan_status'][1:-1]).mean()\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print('Mean dari korelasi tiap variabel yang ada adalah ' + color.BOLD + '{0}'.format(bataskor) + color.END)\n",
        "print('Sehingga,' + color.BOLD + color.DARKCYAN + ' variabel yang dipilih sebagai hasil seleksi fitur adalah' + color.END)\n",
        "hasilseleksi = abs(korelasi['loan_status'][1:-1])[abs(korelasi['loan_status']) > bataskor]\n",
        "print(hasilseleksi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3X5_hgIh17J8"
      },
      "outputs": [],
      "source": [
        "'person_income', 'loan_grade', 'loan_int_rate', 'loan_percent_income', 'person_home_ownership_MORTGAGE', 'person_home_ownership_RENT', 'cb_person_default_on_file_N', 'cb_person_default_on_file_Y'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OabZhLDk6oDr"
      },
      "source": [
        "# Feature Selection dengan Backward Elimination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swPXi8f-17KI"
      },
      "outputs": [],
      "source": [
        "# yang tadi pearsonn skrg pake backward FS nya.\n",
        "# set X and y\n",
        "X = df[['person_income', 'loan_grade', 'loan_int_rate', 'loan_percent_income',\n",
        "        'person_home_ownership_MORTGAGE', 'person_home_ownership_RENT', 'cb_person_default_on_file_N',\n",
        "        'cb_person_default_on_file_Y']]\n",
        "y = df['loan_status']\n",
        "\n",
        "# train - test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "import pandas as pd # import pandas\n",
        "import numpy as np #import numpy\n",
        "import statsmodels.api as sm # import statsmodels\n",
        "\n",
        "# dengan menggunakan formula Backward Regre Elimination\n",
        "def backward_regression(X, y,\n",
        "                           initial_list =[ ],\n",
        "                           threshold_in = 0.01,\n",
        "                           threshold_out = 0.05,\n",
        "                           verbose = True):\n",
        "    included = list(X.columns)\n",
        "    while True:\n",
        "        changed = False\n",
        "        # Fix: Convert to numeric types and ensure proper DataFrame format\n",
        "        X_temp = pd.DataFrame(X[included], dtype=float)\n",
        "        model = sm.OLS(y, sm.add_constant(X_temp)).fit()\n",
        "        # Fix: Access pvalues correctly - they're a Series, not a DataFrame\n",
        "        pvalues = model.pvalues[1:]  # Remove the constant term\n",
        "        worst_pval = pvalues.max()\n",
        "        if worst_pval > threshold_out:\n",
        "            changed = True\n",
        "            worst_feature = pvalues.idxmax()\n",
        "            included.remove(worst_feature)\n",
        "            if verbose:\n",
        "                # Fix: Add placeholders in the format string\n",
        "                print('Drop {} with p-value {}'.format(worst_feature, worst_pval))\n",
        "        if not changed:\n",
        "            break\n",
        "    return included\n",
        "\n",
        "backward_regression(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRJX36IL6ItV"
      },
      "source": [
        "# Splitting dan Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMtML99y17J9"
      },
      "outputs": [],
      "source": [
        "# set X and y\n",
        "X = df[['person_income', 'loan_grade', 'loan_int_rate', 'loan_percent_income',\n",
        "        'person_home_ownership_MORTGAGE', 'person_home_ownership_RENT', 'cb_person_default_on_file_N',\n",
        "        'cb_person_default_on_file_Y']]\n",
        "y = df['loan_status']\n",
        "\n",
        "# train - test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "# MinmMaxScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler()\n",
        "X_train = sc.fit_transform(X_train) # fit X_train\n",
        "X_test = sc.transform(X_test) #transform X_test\n",
        "#save scaler\n",
        "joblib.dump(sc, 'scaler.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIILvwDp6BAL"
      },
      "source": [
        "# Tuning Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE9f7IWu17J-"
      },
      "outputs": [],
      "source": [
        "# hypertuning for LogisticRegression, DecisionTreeClassifier, and SVC\n",
        "# import model yang ingin digunakan\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# memberikan pilihan model beserta parameternya masing-masing\n",
        "model_params = {\n",
        "    'Logistic_Regression': {\n",
        "        'model': LogisticRegression(),\n",
        "        'params' : {\n",
        "            'penalty':['l1', 'l2', 'elasticnet', 'none'],\n",
        "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "            'multi_class': ['auto', 'ovr', 'multinomial'],\n",
        "            'C' : [0.1,8,0.1]\n",
        "        }\n",
        "    },\n",
        "    'decision_tree':{\n",
        "        'model':DecisionTreeClassifier(),\n",
        "        'params':{\n",
        "            'splitter':['best','random'],\n",
        "            'max_features': ['auto', 'sqrt', 'log2'],\n",
        "            'max_depth' : [4,5,6,7,8],\n",
        "            'criterion' :['gini', 'entropy']\n",
        "        }\n",
        "    },\n",
        "    'SVM_Classifier':{\n",
        "        'model' : SVC(),\n",
        "        'params':{\n",
        "            'kernel' : ['rbf','poly','sigmoid','linear'],\n",
        "            'gamma' : ['scale','auto'],\n",
        "        }\n",
        "    }\n",
        "    }\n",
        "scores = []\n",
        "\n",
        "# scoring model and parameters, pencarian model dan parameter terbaik dengan menggunakan GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "for model_name, mp in model_params.items():\n",
        "    clf =  GridSearchCV(mp['model'], mp['params'], cv=3, return_train_score=False)\n",
        "    clf.fit(X_train,y_train) # fit GridSearch ke X_train dan y_train\n",
        "    scores.append({\n",
        "        'model': model_name,\n",
        "        'best_score': clf.best_score_,\n",
        "        'best_params': clf.best_params_\n",
        "    })\n",
        "best = pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
        "best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyYHcqSd17KB"
      },
      "outputs": [],
      "source": [
        "#Best Parameter Logistic Regression\n",
        "best.best_params[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vObyfHD017KC"
      },
      "outputs": [],
      "source": [
        "#Best Parameter Decision Tree\n",
        "best.best_params[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P09j9o2217KD"
      },
      "outputs": [],
      "source": [
        "#Best SVM Classifier\n",
        "best.best_params[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joIa21ZoUZny"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SUb3B4B57GG"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrW9Qclu17KE"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression # import LogisticRegression\n",
        "LR = LogisticRegression(C=0.1, solver='saga', multi_class='ovr', penalty=None, max_iter=1000).fit(X_train, y_train)\n",
        " # membentuk model\n",
        "\n",
        "yhat = LR.predict(X_test)\n",
        "\n",
        "# classification report\n",
        "print(color.BOLD + color.DARKCYAN + \"Classification Report Logistic Regression Model\"+color.END)\n",
        "from sklearn.metrics import classification_report # import classification_report\n",
        "print (classification_report(y_test, yhat)) # mencetak classification report\n",
        "\n",
        "print(color.BOLD +\"f1-score: \"+color.END,f1_score(y_test, yhat, pos_label=0))\n",
        "print(color.BOLD +\"accuracy-score: \"+color.END,accuracy_score(y_test, yhat))\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "conf_matrix = confusion_matrix(y_true=y_test, y_pred=yhat)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
        "\n",
        "print(\"\")\n",
        "print(color.BOLD + color.DARKCYAN + \"Confusion Matrix Logistic Regression Model\"+color.END)\n",
        "plt.xlabel('Predictions', fontsize=18)\n",
        "plt.ylabel('Actuals', fontsize=18)\n",
        "plt.title('Confusion Matrix', fontsize=18)\n",
        "plt.show()\n",
        "#SAVE MODEL\n",
        "joblib.dump(LR, 'logistic_regression_model.pkl')\n",
        "print(\"Model saved as 'logistic_regression_model.pkl'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL4wokPJ6dPu"
      },
      "source": [
        "# Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJS5bbWO17KF"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "credittree = DecisionTreeClassifier(criterion = 'entropy', max_depth = 8, random_state = 0, splitter = 'best')\n",
        "credittree.fit(X_train, y_train)\n",
        "\n",
        "# prediction\n",
        "y_pred = credittree.predict(X_test)\n",
        "\n",
        "# classification report\n",
        "from sklearn.metrics import classification_report\n",
        "print(color.BOLD + color.DARKCYAN + \"Classification Report Decision Tree Model\"+color.END)\n",
        "print(classification_report(y_test,y_pred))\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "print(color.BOLD +\"f1-score: \"+color.END,f1_score(y_test, y_pred, pos_label=0))\n",
        "print(color.BOLD +\"accuracy-score: \"+color.END,accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
        "\n",
        "print(\"\")\n",
        "print(color.BOLD + color.DARKCYAN + \"Confusion Matrix Decision Tree Model\"+color.END)\n",
        "plt.xlabel('Predictions', fontsize=18)\n",
        "plt.ylabel('Actuals', fontsize=18)\n",
        "plt.title('Confusion Matrix', fontsize=18)\n",
        "plt.show()\n",
        "#SAVE MODEL\n",
        "joblib.dump(credittree, 'decision_tree_model.pkl')\n",
        "print(\"Model saved as 'decision_tree_model.pkl'\")\n",
        "\n",
        "#TN = 4977, TP = 876, FN = 476 , FP = 186\n",
        "#Precision = TP/(TP+FP)\n",
        "#Recall = TP/(TP+FN)\n",
        "#1/F1-score = 1/2(1/Precision + 1/Recall) F1-Score adalah harmonic mean dari precision dan recall\n",
        "#Accuracy ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGsOOk3q6fJp"
      },
      "source": [
        "# Support Vector Machines (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DsLSwQB17KH"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "from sklearn import svm\n",
        "import pandas as pd\n",
        "\n",
        "clf = svm.SVC(C=1, gamma = 'scale', kernel='poly')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "yhat = clf.predict(X_test)\n",
        "\n",
        "# classification report\n",
        "from sklearn.metrics import classification_report # import classification_report\n",
        "print(color.BOLD + color.DARKCYAN + \"Classification Report SVM Model\"+color.END)\n",
        "print (classification_report(y_test, yhat)) # mencetak classification report\n",
        "\n",
        "print(color.BOLD +\"f1-score: \"+color.END,f1_score(y_test, yhat, pos_label=0))\n",
        "print(color.BOLD +\"accuracy-score: \"+color.END,accuracy_score(y_test, yhat))\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "conf_matrix = confusion_matrix(y_true=y_test, y_pred=yhat)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
        "\n",
        "print(\"\")\n",
        "print(color.BOLD + color.DARKCYAN + \"Confusion Matrix SVM Model\"+color.END)\n",
        "plt.xlabel('Predictions', fontsize=18)\n",
        "plt.ylabel('Actuals', fontsize=18)\n",
        "plt.title('Confusion Matrix', fontsize=18)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HapT72kzRGQa"
      },
      "source": [
        "# Testing Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AupllqaeSzwg"
      },
      "outputs": [],
      "source": [
        "y_test2=y_test.to_numpy()\n",
        "dftest = [y_test2,y_pred]\n",
        "\n",
        "dfframe = pd.DataFrame (dftest).transpose()\n",
        "dfframe.columns = ['y_test', 'y_pred']\n",
        "dfframe.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYIlqqx5NFSI"
      },
      "outputs": [],
      "source": [
        "loan_grade_dict = {\n",
        "  \"A\": 0,\n",
        "  \"B\": 1,\n",
        "  \"C\": 2,\n",
        "  \"D\": 3,\n",
        "  \"E\": 4,\n",
        "  \"F\": 5,\n",
        "  \"G\": 6 }\n",
        "\n",
        "X = df[['person_income', 'loan_grade', 'loan_int_rate', 'loan_percent_income',\n",
        "        'person_home_ownership_MORTGAGE', 'person_home_ownership_RENT', 'cb_person_default_on_file_N',\n",
        "        'cb_person_default_on_file_Y']]\n",
        "y = df['loan_status']\n",
        "\n",
        "print(color.BOLD + color.DARKCYAN + \"Credit Risk Prediction Programming Algorithm\"+color.END)\n",
        "\n",
        "\n",
        "person_income = int(input(\"Person Income: \"))\n",
        "loan_grade = str(input(\"Loan Grade: \")).upper()\n",
        "loan_int_rate = float(input(\"Loan Interest Rate: \"))\n",
        "loan_percent_income = float(input(\"Loan Percent Income: \"))\n",
        "person_home_ownership = str(input(\"Person Home Ownership: \")).upper()\n",
        "cb_person_default_on_file = str(input(\"CB Person Default on File: \")).upper()\n",
        "\n",
        "if person_home_ownership == 'MORTGAGE':\n",
        "  person_home_ownership_MORTGAGE=1\n",
        "  person_home_ownership_RENT=0\n",
        "elif person_home_ownership == 'RENT':\n",
        "  person_home_ownership_MORTGAGE=0\n",
        "  person_home_ownership_RENT=1\n",
        "else:\n",
        "  person_home_ownership_MORTGAGE=0\n",
        "  person_home_ownership_RENT=0\n",
        "\n",
        "if cb_person_default_on_file == 'Y':\n",
        "  cb_person_default_on_file_Y=1\n",
        "  cb_person_default_on_file_N=0\n",
        "else:\n",
        "  cb_person_default_on_file_Y=0\n",
        "  cb_person_default_on_file_N=1\n",
        "\n",
        "x_data = []\n",
        "x_data.append(person_income)\n",
        "x_data.append(loan_grade_dict[loan_grade])\n",
        "x_data.append(loan_int_rate)\n",
        "x_data.append(loan_percent_income)\n",
        "x_data.append(person_home_ownership_MORTGAGE)\n",
        "x_data.append(person_home_ownership_RENT)\n",
        "x_data.append(cb_person_default_on_file_N)\n",
        "x_data.append(cb_person_default_on_file_Y)\n",
        "\n",
        "import numpy as np\n",
        "x_data_array=np.array(x_data)\n",
        "x_data_reshape=x_data_array.reshape(1,-1)\n",
        "\n",
        "# MinmMaxScaler\n",
        "x_data_fit = sc.transform(x_data_reshape) #transform X_test\n",
        "\n",
        "y_pred = credittree.predict(x_data_fit)\n",
        "\n",
        "print(color.BOLD + \"Kelayakan Penerima Nasabah Kredit: \"+color.END)\n",
        "if y_pred[0]==0:\n",
        "  print(color.BOLD + color.BLUE + \"Layak\"+color.END)\n",
        "else:\n",
        "  print(color.BOLD + color.RED + \"Tidak Layak\"+color.END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYGpAgSH7YXS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "kK-PavMSUvxN",
        "2GOFUIuLUrh-",
        "GxfqraomUXrw"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "anaconda-panel-2023.05-py310",
      "language": "python",
      "name": "conda-env-anaconda-panel-2023.05-py310-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}